{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Retry.__init__() got an unexpected keyword argument 'method_whitelist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m captures \u001b[38;5;241m=\u001b[39m wayback\u001b[38;5;241m.\u001b[39msnapshots()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Setup retry strategy\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m retry_strategy \u001b[38;5;241m=\u001b[39m Retry(\n\u001b[1;32m     33\u001b[0m     total\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     34\u001b[0m     backoff_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     35\u001b[0m     status_forcelist\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m429\u001b[39m, \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m502\u001b[39m, \u001b[38;5;241m503\u001b[39m, \u001b[38;5;241m504\u001b[39m],\n\u001b[1;32m     36\u001b[0m     method_whitelist\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPTIONS\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m adapter \u001b[38;5;241m=\u001b[39m HTTPAdapter(max_retries\u001b[38;5;241m=\u001b[39mretry_strategy)\n\u001b[1;32m     39\u001b[0m http \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mSession()\n",
      "\u001b[0;31mTypeError\u001b[0m: Retry.__init__() got an unexpected keyword argument 'method_whitelist'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "from waybackpy import WaybackMachineCDXServerAPI\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Define the URL of the website you want to scrape\n",
    "url = \"http://cesta.stanford.edu\"\n",
    "\n",
    "# Create the folder for storing HTML files if it doesn't exist\n",
    "folder_name = \"wayback_html\"\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# Create or open the CSV file for writing the date and URL\n",
    "csv_filename = \"scraped_wayback_urls.csv\"\n",
    "file_exists = os.path.isfile(csv_filename)\n",
    "\n",
    "with open(csv_filename, mode='a', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    if not file_exists:\n",
    "        # Write the header only if the file doesn't exist\n",
    "        csv_writer.writerow([\"Date\", \"URL\"])\n",
    "\n",
    "    # Step 1: Get all available captures from Wayback Machine\n",
    "    wayback = WaybackMachineCDXServerAPI(url)\n",
    "    captures = wayback.snapshots()\n",
    "\n",
    "    # Setup retry strategy\n",
    "    retry_strategy = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    http = requests.Session()\n",
    "    http.mount(\"https://\", adapter)\n",
    "    http.mount(\"http://\", adapter)\n",
    "\n",
    "    # Step 2: Iterate through each capture, scrape the HTML, and save it\n",
    "    for capture in captures:\n",
    "        archive_url = capture.archive_url\n",
    "        date_part = capture.timestamp\n",
    "\n",
    "        try:\n",
    "            # Scrape the archived website\n",
    "            response = http.get(archive_url, timeout=10)\n",
    "            response.raise_for_status()  # Raise an HTTPError for bad responses\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                html_content = response.text\n",
    "\n",
    "                # Save the HTML content to a file named with the date\n",
    "                html_filename = f\"{folder_name}/{date_part}.html\"\n",
    "                with open(html_filename, \"w\", encoding='utf-8') as html_file:\n",
    "                    html_file.write(html_content)\n",
    "\n",
    "                # Save the date and URL to the CSV file\n",
    "                csv_writer.writerow([date_part, archive_url])\n",
    "\n",
    "                print(f\"Successfully saved HTML content and updated CSV file for {date_part}.\")\n",
    "            else:\n",
    "                print(f\"Failed to retrieve the page for {date_part}. Status code: {response.status_code}\")\n",
    "\n",
    "        except (requests.exceptions.SSLError, requests.exceptions.ConnectionError) as e:\n",
    "            print(f\"SSL or connection error for {date_part}: {e}\")\n",
    "        except requests.exceptions.Timeout as e:\n",
    "            print(f\"Timeout error for {date_part}: {e}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request exception for {date_part}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cesta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
